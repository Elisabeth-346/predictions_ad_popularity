# Необходимые импорты
import pandas as pd
import numpy as np
import random
import csv
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, roc_auc_score
from sklearn.compose import make_column_selector, ColumnTransformer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, PowerTransformer
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.calibration import CalibratedClassifierCV
from sklearn.decomposition import PCA
from datetime import datetime
from scipy import stats
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.class_weight import compute_sample_weight
from imblearn.over_sampling import SMOTE
import joblib
import warnings
import os

# Дополнительный настройки
warnings.filterwarnings("ignore")
pd.set_option('display.max_columns', None)
sns.set_style("whitegrid")

SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# Загрузка CSV-файлов
def robust_csv_loader(file_path, delimiter=None):
    """Загрузка CSV с обработкой возможных ошибок формата."""
    print(f"Загрузка файла: {file_path}")
    
    possible_delimiters = [';', ','] if delimiter is None else [delimiter]
    
    for delim in possible_delimiters:
        try:
            df = pd.read_csv(file_path, delimiter=delim, on_bad_lines='warn', engine='python', encoding='utf-8')
            if len(df.columns) > 1 and len(df) > 0:
                print(f"Успешно загружено с разделителем '{delim}'")
                return df
        except Exception as e:
            print(f"Ошибка с разделителем '{delim}': {e}")
    
    print("Пропускаем неправильные строки...")
    for delim in possible_delimiters:
        try:
            df = pd.read_csv(file_path, delimiter=delim, on_bad_lines='skip', engine='python', encoding='utf-8')
            if len(df.columns) > 1 and len(df) > 0:
                print(f"Загружено с пропуском ошибок: {len(df)} строк")
                return df
        except:
            continue
    
    return pd.DataFrame()

# Обработка данных
def safe_load_data():
    """
    Безопасная загрузка данных с обработкой ошибок
    """
    train_path = "ads.csv"
    test_path = "test.csv"
    
    if not os.path.exists(train_path):
        raise FileNotFoundError(f"Файл {train_path} не найден")
    
    print("ЗАГРУЗКА ОБУЧАЮЩИХ ДАННЫХ...")
    train = robust_csv_loader(train_path, ';')
    
    if train.empty:
        raise ValueError("Не удалось загрузить обучающие данные")
    
    print(f"Колонки в данных: {list(train.columns)}")
    print(f"Размер данных: {train.shape}")
    print("Первые 3 строки данных:\n", train.head(3))
    
    # Целевая переменной
    target_candidates = ['successful', 'target', 'label', 'y', 'class', 'result']
    target_column = next((c for c in target_candidates if c in train.columns), train.columns[-1])
    print(f"Целевая переменная: {target_column}")
    
    # Загрузка тестовых данных
    if os.path.exists(test_path):
        print("\nЗАГРУЗКА ТЕСТОВЫХ ДАННЫХ...")
        test = robust_csv_loader(test_path, ';')
        print(f"Тестовые данные: {test.shape}\nПервые 3 строки:\n", test.head(3))
    else:
        print("Тестовые данные не найдены")
        test = pd.DataFrame()
    
    return train, test, target_column

def preprocess_data(train, test, target_col):
    """
    Подготовка данных с дополнительной обработкой и очищением
    """
    original_idx = train.index
    
    
    if test is None:
        test = pd.DataFrame()
    
    
    train = train.drop_duplicates().dropna(how='all')
    if not test.empty:
        test = test.drop_duplicates().dropna(how='all')
    
    print(f"После удаления дубликатов: {len(train)} строк")
    
    
    for df in [train, test]:
        if not df.empty:
            num_cols = df.select_dtypes(include=np.number).columns
            for col in num_cols:
                if col != target_col and df[col].isnull().sum() > 0:
                    df[col] = df[col].fillna(df[col].median())
    
    
    for df in [train, test]:
        if not df.empty:
            cat_cols = df.select_dtypes(exclude=np.number).columns
            for col in cat_cols:
                
                df[col] = df[col].apply(lambda x: np.mean(list(map(float, x.split(',')))) if isinstance(x, str) and ',' in x else x)
    
    
    X = train.drop(columns=[target_col])
    y = train[target_col]
    
    
    if y.dtype == 'object':
        try:
            y = pd.to_numeric(y, errors='coerce')
            valid_mask = y.notna()
            X = X.loc[valid_mask]
            y = y.loc[valid_mask]
            print(f"После преобразования целевой переменной: {len(X)} строк")
        except:
            y = y.astype('category').cat.codes
            print("Целевая переменная преобразована в категориальные коды")
    
   
    if not test.empty:
        
        test_ids = test['id'].values.copy()  
        test = test.drop(columns=['id'])     
       
        if "Unnamed: 0" not in test.columns:
            test["Unnamed: 0"] = np.arange(len(test))
        
        test = test[X.columns]
    
    
    processed_test = test
    
    print("ПРЕДОБРАБОТКА ЗАВЕРШЕНА")
    print(f"Фичевые колонки: {list(X.columns)}")
    return X, y, processed_test, test_ids

# Aнализ данных
def analyze_data(X, y):
    """
    Анализ данных и вывод базовой статистики
    """
    print("\n== АНАЛИЗ ДАННЫХ ==")
    print(f"Размер данных: {X.shape}")
    print(f"Тип целевой переменной: {y.dtype}")
    
    print("\nРаспределение классов:")
    class_dist = y.value_counts(normalize=True)
    print(class_dist)
    
    plt.figure(figsize=(8, 6))
    class_dist.plot(kind='bar')
    plt.title('Распределение классов')
    plt.xlabel('Класс')
    plt.ylabel('Доля')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    # Анализ числовых признаков
    num_cols = X.select_dtypes(include=np.number).columns
    if len(num_cols) > 0:
        corr_matrix = pd.concat([X[num_cols], y.rename('target')], axis=1).corr()
        target_corr = corr_matrix['target'].drop('target').sort_values(key=abs, ascending=False)
        
        print("\nТоп-10 признаков по абсолютной корреляции:")
        print(target_corr.head(10))
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', center=0)
        plt.title("Матрица корреляций")
        plt.tight_layout()
        plt.show()
    else:
        print("\nНет числовых признаков для анализа корреляций")

# Главная функция
def main():
    print("=== ЗАПУСК ЭКСПЕРИМЕНТА ===")
    
    # 1. Загрузка данных
    train, test, target_col = safe_load_data()
    
    if train.empty or target_col is None:
        print("Ошибка: Не удалось загрузить данные или определить целевую переменную")
        return
    
    # 2. Расширенная предобработка
    X, y, X_test, test_ids = preprocess_data(train, test, target_col)
    if X.empty or y.empty:
        print("Ошибка: Проблемы с предобработкой данных")
        return
    
    print(f"\nУСПЕШНО ЗАГРУЖЕНО:")
    print(f"Обучающие данные: {X.shape}")
    print(f"Тестовые данные: {X_test.shape if not X_test.empty else 'Нет'}")
    
    # 3. Балансировка классов (при необходимости)
    class_balance = y.value_counts(normalize=True)
    print(f"Баланс классов: {class_balance.to_dict()}")
    
    if class_balance.min() < 0.3:
        print("Применяем балансировку классов...")
        X, y = balance_classes(X, y)
        print(f"После балансировки: {X.shape}")
    
    # 4. Анализ данных
    analyze_data(X, y)
    
    # 5. Разделение на train/validation
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, 
        test_size=0.2,
        random_state=SEED,
        stratify=y
    )
    
    print(f"Разделение данных: Train {X_train.shape}, Val {X_val.shape}")
    
    # 6. Выбор модели и настройка гиперпараметров
    print("\n=== НАСТРОЙКА ГИПЕРПАРАМЕТРОВ ===")
    # Оптимизация гиперпараметров с использованием GridSearchCV
    rf_model = RandomForestClassifier(random_state=SEED)
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    print(f"Лучшая модель: {best_model}")
    
    # 7. Прогнозирование и оценка на валидационном наборе
    print("\n=== ПРОГНОЗИРОВАНИЕ НА VALIDATION SET ===")
    val_predictions = best_model.predict(X_val)
    val_accuracy = accuracy_score(y_val, val_predictions)
    print(f"\nФинальные результаты на validation set:")
    print(f"Validation Accuracy: {val_accuracy:.4f}")
    print("Classification Report:")
    print(classification_report(y_val, val_predictions))
    
    # 8. Экспорт прогнозов
    if not X_test.empty and test_ids is not None:
        print("\n== ПОДГОТОВКА К ПРЕДОСКАЗАНИЮ ==")
        print("Структура тестовых данных:")
        print(X_test.info())
        print("Пример тестовых данных:")
        print(X_test.head())
        
        try:
            test_predictions = best_model.predict(X_test)
            print("Размеры предсказанных классов:", test_predictions.shape)
            print("Пример предсказанных классов:", test_predictions[:5])
            
            # Создаем DataFrame с id и прогнозами
            submission_df = pd.DataFrame({
                'id': test_ids,                   # Сюда попали идентификаторы из тестового набора
                'prediction': test_predictions
            })
            
            # Сохраняем результаты в файл
            submission_path = os.path.abspath('submission.csv')
            submission_df.to_csv(submission_path, index=False, encoding='utf-8')
            print("Файл 'submission.csv' успешно сохранен.")
            
        except Exception as e:
            print(f"Ошибка при предсказании на тестовых данных: {e}")
    
    # 9. Сохранение модели
    try:
        joblib.dump(best_model, 'best_model.pkl')
        print("Модель сохранена как 'best_model.pkl'")
    except Exception as e:
        print(f"Ошибка при сохранении модели: {str(e)}")

    print("\n=== ЭКСПЕРИМЕНТ ЗАВЕРШЕН ===")
    print(f"Validation Accuracy: {val_accuracy:.4f}")


# Запуск основной функции
if __name__ == "__main__":
    main()                                                                                                                                                                             
