import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, roc_auc_score
from sklearn.compose import make_column_selector, ColumnTransformer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, PowerTransformer
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.calibration import CalibratedClassifierCV
from sklearn.decomposition import PCA
import joblib
import warnings
import os
import csv
from scipy import stats

# Настройки
warnings.filterwarnings("ignore")
pd.set_option('display.max_columns', None)
sns.set_style("whitegrid")

SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# Функция для надежной загрузки CSV с проблемным форматом
def robust_csv_loader(file_path, delimiter=None):
    """Загрузка CSV с обработкой ошибок формата"""
    print(f"Загрузка файла: {file_path}")
    
    possible_delimiters = [';', ',', '\t', '|'] if delimiter is None else [delimiter]
    
    for delim in possible_delimiters:
        try:
            df = pd.read_csv(file_path, delimiter=delim, on_bad_lines='warn', engine='python', encoding='utf-8')
            if len(df.columns) > 1 and len(df) > 0:
                print(f"Успешно загружено с разделителем '{delim}'")
                return df
        except Exception as e:
            print(f"Ошибка с разделителем '{delim}': {e}")
    
    # Если обычные методы не сработали, пробуем пропустить ошибки
    print("Пробуем загрузку с пропуском ошибочных строк...")
    for delim in possible_delimiters:
        try:
            df = pd.read_csv(file_path, delimiter=delim, on_bad_lines='skip', engine='python', encoding='utf-8')
            if len(df.columns) > 1 and len(df) > 0:
                print(f"Загружено с пропуском ошибок: {len(df)} строк")
                return df
        except:
            continue
    
    return pd.DataFrame()

# Безопасная загрузка данных
def safe_load_data():
    """Безопасная загрузка данных с обработкой ошибок"""
    train_path = "ads.csv"
    test_path = "test.csv"
    
    if not os.path.exists(train_path):
        raise FileNotFoundError(f"Файл {train_path} не найден")
    
    print("ЗАГРУЗКА ОБУЧАЮЩИХ ДАННЫХ...")
    train = robust_csv_loader(train_path, ';')
    
    if train.empty:
        raise ValueError("Не удалось загрузить обучающие данные")
    
    print(f"Колонки в данных: {list(train.columns)}")
    print(f"Размер данных: {train.shape}")
    print("Первые 3 строки данных:\n", train.head(3))
    
    # Определяем целевую переменную
    target_candidates = ['successful', 'target', 'label', 'y', 'class', 'result']
    target_column = next((c for c in target_candidates if c in train.columns), train.columns[-1])
    print(f"Целевая переменная: {target_column}")
    
    # Загрузка тестовых данных
    if os.path.exists(test_path):
        print("\nЗАГРУЗКА ТЕСТОВЫХ ДАННЫХ...")
        test = robust_csv_loader(test_path, ';')
        print(f"Тестовые данные: {test.shape}\nПервые 3 строки:\n", test.head(3))
    else:
        print("Тестовые данные не найдены")
        test = pd.DataFrame()
    
    return train, test, target_column

# Балансировка классов методом over-sampling
def balance_classes(X, y):
    """Балансировка классов путём увеличения меньшинства"""
    from collections import Counter
    
    counts = Counter(y)
    print(f"Исходное распределение классов: {counts}")
    
    if len(counts) < 2:
        return X, y
    
    major_class = max(counts, key=counts.get)
    minor_class = min(counts, key=counts.get)
    max_count = counts[major_class]
    
    indices = []
    for label in counts.keys():
        idx = np.where(y == label)[0]
        if label == minor_class:
            idx = np.random.choice(idx, max_count, replace=True)
        indices.extend(idx)
    
    balanced_X = X.iloc[indices]
    balanced_y = y.iloc[indices]
    
    print(f"После балансировки: {Counter(balanced_y)}")
    return balanced_X, balanced_y

# Расширенная предобработка данных
def preprocess_data(train, test, target_col):
    """Подготовка данных с дополнительной обработкой и очищением"""
    original_idx = train.index
    
    # Удаляем дублирующиеся записи и пустые строки
    train = train.drop_duplicates().dropna(how='all')
    if not test.empty:
        test = test.drop_duplicates().dropna(how='all')
    
    print(f"После удаления дубликатов: {len(train)} строк")
    
    # Исключаем лишние столбцы
    cols_to_remove = ['Unnamed: 0', 'unnamed: 0', 'index']
    for col in cols_to_remove:
        if col in train.columns and col != target_col:
            train = train.drop(columns=[col])
            print(f"Удалена колонка из train: {col}")
        if not test.empty and col in test.columns and col != target_col:
            test = test.drop(columns=[col])
            print(f"Удалена колонка из test: {col}")
    
    # Конвертирование колонки publishers в числовую форму
    def convert_publishers(df, col_name='publishers'):
        if col_name not in df.columns:
            return df
        
        temp_df = df.copy()
        unique_publishers = set()
        for pub_list in temp_df[col_name].astype(str):
            unique_publishers.update(pub.strip() for pub in pub_list.split(',') if pub.strip())
        
        for pub in sorted(unique_publishers):
            new_col = f'publisher_{pub}'
            temp_df[new_col] = temp_df[col_name].apply(lambda x: 1 if pub in str(x).split(',') else 0)
        
        temp_df = temp_df.drop(columns=[col_name])
        return temp_df
    
    train = convert_publishers(train, 'publishers')
    if not test.empty and 'publishers' in test.columns:
        test = convert_publishers(test, 'publishers')
    
    # Обрабатываем отсутствующие значения
    for df in [train, test]:
        if not df.empty:
            num_cols = df.select_dtypes(include=np.number).columns
            for col in num_cols:
                if col != target_col and df[col].isnull().sum() > 0:
                    df[col] = df[col].fillna(df[col].median())
    
    # Преобразовываем объектные типы в числовые
    for df in [train, test]:
        if not df.empty:
            obj_cols = df.select_dtypes(include='object').columns
            for col in obj_cols:
                if col != target_col:
                    try:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        print(f"Преобразовано {col} в числовой формат")
                    except:
                        df = df.drop(columns=[col])
                        print(f"Удалена неконвертируемая колонка: {col}")
    
    # Генерируем новые признаки
    num_cols = train.select_dtypes(include=np.number).columns
    num_cols = [col for col in num_cols if col != target_col]
    
    if len(num_cols) > 0:
        for df in [train, test]:
            if not df.empty:
                for col in num_cols[:min(3, len(num_cols))]:
                    if col in df.columns:
                        if (df[col] >= 0).all():
                            df[f'{col}_log'] = np.log1p(df[col])
                        df[f'{col}_squared'] = df[col] ** 2
    
    # Обрабатываем выбросы
    for col in num_cols:
        if col in train.columns:
            q1 = train[col].quantile(0.25)
            q3 = train[col].quantile(0.75)
            iqr = q3 - q1
            lb = q1 - 1.5 * iqr
            ub = q3 + 1.5 * iqr
            
            train[col] = np.clip(train[col], lb, ub)
            if not test.empty and col in test.columns:
                test[col] = np.clip(test[col], lb, ub)
    
    # Формируем массив признаков и целевую переменную
    X = train.drop(columns=[target_col])
    y = train[target_col]
    
    # Преобразуем целевую переменную
    if y.dtype == 'object':
        try:
            y = pd.to_numeric(y, errors='coerce')
            valid_mask = y.notna()
            X = X.loc[valid_mask]
            y = y.loc[valid_mask]
            print(f"После преобразования целевой переменной: {len(X)} строк")
        except:
            y = y.astype('category').cat.codes
            print("Целевая переменная преобразована в категориальные коды")
    
    # Тестовые данные
    processed_test = test.drop(columns=[target_col]) if not test.empty and target_col in test.columns else test
    test_ids = test['id'].copy() if not test.empty and 'id' in test.columns else None
    
    print("ПРЕДОБРАБОТКА ЗАВЕРШЕНА")
    print(f"Фичевые колонки: {list(X.columns)}")
    return X, y, processed_test, test_ids

def analyze_data(X, y):
    """Анализ данных с построением графиков и отображением статистик"""
    print("\n== АНАЛИЗ ДАННЫХ ==")
    print(f"Размер данных: {X.shape}")
    print(f"Тип целевой переменной: {y.dtype}")
    
    print("\nРаспределение классов:")
    class_dist = y.value_counts(normalize=True)
    print(class_dist)
    
    plt.figure(figsize=(8, 6))
    class_dist.plot(kind='bar')
    plt.title('Распределение классов')
    plt.xlabel('Класс')
    plt.ylabel('Доля')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    # Анализ числовых признаков
    num_cols = X.select_dtypes(include=np.number).columns
    if len(num_cols) > 0:
        corr_matrix = pd.concat([X[num_cols], y.rename('target')], axis=1).corr()
        target_corr = corr_matrix['target'].drop('target').sort_values(key=abs, ascending=False)
        
        print("\nТоп-10 признаков по абсолютной корреляции:")
        print(target_corr.head(10))
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', center=0)
        plt.title("Матрица корреляций")
        plt.tight_layout()
        plt.show()
    else:
        print("\nНет числовых признаков для анализа корреляций")

# Создание препроцессора
def create_preprocessor():
    """Создание препроцессора для нормализации данных"""
    transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),  # Заполняем недостающие значения медианой
        ('scaler', RobustScaler())                     # Масштабируем данные
    ])
    
    preprocessor = ColumnTransformer(transformers=[
        ('num', transformer, make_column_selector(dtype_include=np.number))  # Работаем только с численными признаками
    ], remainder='drop')
    
    return preprocessor

# Получение единственной модели логистической регрессии
def get_logreg_model():
    """Получение логистической регрессии с параметрами для GridSearch"""
    lr_model = LogisticRegression(max_iter=1000, random_state=SEED)
    params = {
        'model__C': np.logspace(-3, 3, 7),  # Диапазон штрафов
        'model__penalty': ['l1', 'l2'],     # Тип регуляризации
        'model__solver': ['liblinear', 'saga'],  # Метод оптимизации
        'model__class_weight': [None, 'balanced']  # Вес классов
    }
    return {'model': lr_model, 'params': params}

# Обучение и оценка логистической регрессии
def train_and_evaluate_logreg(X, y, preprocessor):
    """Обучение и оценка логистической регрессии с GridSearchCV"""
    model_cfg = get_logreg_model()
    model = model_cfg['model']
    params = model_cfg['params']
    
    pipe = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    
    # Кросс-валидация
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)
    grid_search = GridSearchCV(
        estimator=pipe,
        param_grid=params,
        cv=cv,
        scoring='accuracy',
        verbose=1,
        n_jobs=-1
    )
    
    grid_search.fit(X, y)
    
    # Лучший результат и параметры
    best_estimator = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    
    # Прогнозы на тренировочных данных
    y_pred = best_estimator.predict(X)
    train_acc = accuracy_score(y, y_pred)
    
    print(f"Лучший результат кросс-валидации: {best_score:.3f}")
    print(f"Лучшие параметры: {best_params}")
    print(f"Accuracy на тренировочных данных: {train_acc:.3f}")
    
    return best_estimator

# Главная функция
def main():
    print("=== ЗАПУСК ЭКСПЕРИМЕНТА ===")
    
    # 1. Загрузка данных
    train, test, target_col = safe_load_data()
    
    if train.empty or target_col is None:
        print("Ошибка: Не удалось загрузить данные или определить целевую переменную")
        return
    
    # 2. Расширенная предобработка
    X, y, X_test, test_ids = preprocess_data(train, test, target_col)
    if X.empty or y.empty:
        print("Ошибка: Проблемы с предобработкой данных")
        return
    
    print(f"\nУСПЕШНО ЗАГРУЖЕНО:")
    print(f"Обучающие данные: {X.shape}")
    print(f"Тестовые данные: {X_test.shape if not X_test.empty else 'Нет'}")
    
    # 3. Балансировка классов (при необходимости)
    class_balance = y.value_counts(normalize=True)
    print(f"Баланс классов: {class_balance.to_dict()}")
    
    if class_balance.min() < 0.3:
        print("Применяем балансировку классов...")
        X, y = balance_classes(X, y)
        print(f"После балансировки: {X.shape}")
    
    # 4. Анализ данных
    analyze_data(X, y)
    
    # 5. Разделение на train/validation
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, 
        test_size=0.2,
        random_state=SEED,
        stratify=y
    )
    
    print(f"Разделение данных: Train {X_train.shape}, Val {X_val.shape}")
    
    # 6. Обучение и оценка логистической регрессии
    print("\n=== ОБУЧЕНИЕ LOGISTIC REGRESSION ===")
    preprocessor = create_preprocessor()
    best_model = train_and_evaluate_logreg(X_train, y_train, preprocessor)
    
    # 7. Оценка на валидационном наборе
    val_predictions = best_model.predict(X_val)
    val_accuracy = accuracy_score(y_val, val_predictions)
    print(f"\nФинальные результаты на validation set:")
    print(f"Validation Accuracy: {val_accuracy:.4f}")
    print("Классификация Report:")
    print(classification_report(y_val, val_predictions))
    
    # 8. Работа с тестовыми данными
    if not X_test.empty and test_ids is not None:
        print("\n=== ОБРАБОТКА ТЕСТОВЫХ ДАННЫХ ===")
        try:
            test_predictions = best_model.predict(X_test)
            
            submission_df = pd.DataFrame({
                'id': test_ids,
                'prediction': test_predictions
            })
            
            submission_df.to_csv('submission_logreg.csv', index=False, encoding='utf-8')
            print("Прогнозы логистической регрессии сохранены как 'submission_logreg.csv'")
            
        except Exception as e:
            print(f"Ошибка при предсказании на тестовых данных: {e}")
    
    # 9. Сохранение модели
    try:
        joblib.dump(best_model, 'best_logreg_model.pkl')
        print("Модель логистической регрессии сохранена как 'best_logreg_model.pkl'")
    except Exception as e:
        print(f"Ошибка при сохранении модели: {str(e)}")

    print("\n=== ЭКСПЕРИМЕНТ ЗАВЕРШЕН ===")
    print(f"Validation Accuracy: {val_accuracy:.4f}")

# Запуск основной функции
if __name__ == "__main__":
    main()                                                                                                                                                                                                   
